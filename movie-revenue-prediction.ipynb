{"cells":[{"cell_type":"markdown","metadata":{"id":"1iNojPmgLLs9"},"source":["# Get Started\n","\n","Alex Schittko\n","\n","Robert Waguespack\n","\n","Run this block to set up the notebook\n","\n","**You must enable Internet in Kaggle before running this notebook!**\n","\n","**This requires you to verify your Kaggle account!**\n","\n","* Download CSV Files test/training sets\n","* Import python dependencies\n","\n","Goal is to [predict box office revenues](https://www.kaggle.com/c/tmdb-box-office-prediction/submit)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"tx3yxnlCHAqu","outputId":"335bc65b-c603-4a53-d5ab-dc45b05be5e1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Ready\n"]}],"source":["output_path = \"./\" # Where are outputs stored in your notebook? (With trailing slash, eg /output/)\n","### input_path = \"../input/tmdb-box-office-prediction/\" # Where can inputs be stored? (With trailing slash, eg /input/)\n","\n","# Uncomment these if the test.csv and train.csv don't exist in INPUT_PATH\n","#!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=13f3n4H67RjbEHPl_A4i9R6oY9jUa2eOm' -O {input_path}test.csv\n","#!wget -q --show-progress --no-check-certificate 'https://docs.google.com/uc?export=download&id=1JxEPMg415Y6NIslXcL9mWGr8RMx86B6Y' -O {input_path}train.csv\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import missingno as mno\n","import math\n","import json\n","from multiprocessing import Pool\n","import multiprocessing\n","from tqdm import tqdm,trange,tqdm_notebook\n","from time import sleep\n","from sklearn.model_selection import train_test_split\n","print(\"Ready\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"n6_FFjK9HBP_","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 4398 entries, 0 to 4397\n","Data columns (total 22 columns):\n"," #   Column                 Non-Null Count  Dtype         \n","---  ------                 --------------  -----         \n"," 0   id                     4398 non-null   int64         \n"," 1   belongs_to_collection  877 non-null    object        \n"," 2   budget                 4398 non-null   int64         \n"," 3   genres                 4382 non-null   object        \n"," 4   homepage               1420 non-null   object        \n"," 5   imdb_id                4398 non-null   object        \n"," 6   original_language      4398 non-null   object        \n"," 7   original_title         4398 non-null   object        \n"," 8   overview               4384 non-null   object        \n"," 9   popularity             4398 non-null   float64       \n"," 10  poster_path            4397 non-null   object        \n"," 11  production_companies   4140 non-null   object        \n"," 12  production_countries   4296 non-null   object        \n"," 13  release_date           4397 non-null   datetime64[ns]\n"," 14  runtime                4394 non-null   float64       \n"," 15  spoken_languages       4356 non-null   object        \n"," 16  status                 4396 non-null   object        \n"," 17  tagline                3535 non-null   object        \n"," 18  title                  4395 non-null   object        \n"," 19  Keywords               4005 non-null   object        \n"," 20  cast                   4385 non-null   object        \n"," 21  crew                   4376 non-null   object        \n","dtypes: datetime64[ns](1), float64(2), int64(2), object(17)\n","memory usage: 756.0+ KB\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>has_homepage</th>\n","      <th>popularity_crew</th>\n","      <th>popularity_cast</th>\n","      <th>index</th>\n","      <th>id</th>\n","      <th>belongs_to_collection</th>\n","      <th>budget</th>\n","      <th>genres</th>\n","      <th>homepage</th>\n","      <th>imdb_id</th>\n","      <th>...</th>\n","      <th>spoken_languages</th>\n","      <th>status</th>\n","      <th>tagline</th>\n","      <th>title</th>\n","      <th>Keywords</th>\n","      <th>cast</th>\n","      <th>crew</th>\n","      <th>revenue</th>\n","      <th>cast_json</th>\n","      <th>crew_json</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>[{'id': 313576, 'name': 'Hot Tub Time Machine ...</td>\n","      <td>14000000</td>\n","      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n","      <td>NaN</td>\n","      <td>tt2637294</td>\n","      <td>...</td>\n","      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n","      <td>Released</td>\n","      <td>The Laws of Space and Time are About to be Vio...</td>\n","      <td>Hot Tub Time Machine 2</td>\n","      <td>[{'id': 4379, 'name': 'time travel'}, {'id': 9...</td>\n","      <td>[{'cast_id': 4, 'character': 'Lou', 'credit_id...</td>\n","      <td>[{'credit_id': '59ac067c92514107af02c8c8', 'de...</td>\n","      <td>12314651.0</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>[{'id': 107674, 'name': 'The Princess Diaries ...</td>\n","      <td>40000000</td>\n","      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n","      <td>NaN</td>\n","      <td>tt0368933</td>\n","      <td>...</td>\n","      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n","      <td>Released</td>\n","      <td>It can take a lifetime to find true love; she'...</td>\n","      <td>The Princess Diaries 2: Royal Engagement</td>\n","      <td>[{'id': 2505, 'name': 'coronation'}, {'id': 42...</td>\n","      <td>[{'cast_id': 1, 'character': 'Mia Thermopolis'...</td>\n","      <td>[{'credit_id': '52fe43fe9251416c7502563d', 'de...</td>\n","      <td>95149435.0</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>3300000</td>\n","      <td>[{'id': 18, 'name': 'Drama'}]</td>\n","      <td>http://sonyclassics.com/whiplash/</td>\n","      <td>tt2582802</td>\n","      <td>...</td>\n","      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n","      <td>Released</td>\n","      <td>The road to greatness can take you to the edge.</td>\n","      <td>Whiplash</td>\n","      <td>[{'id': 1416, 'name': 'jazz'}, {'id': 1523, 'n...</td>\n","      <td>[{'cast_id': 5, 'character': 'Andrew Neimann',...</td>\n","      <td>[{'credit_id': '54d5356ec3a3683ba0000039', 'de...</td>\n","      <td>13092000.0</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>1200000</td>\n","      <td>[{'id': 53, 'name': 'Thriller'}, {'id': 18, 'n...</td>\n","      <td>http://kahaanithefilm.com/</td>\n","      <td>tt1821480</td>\n","      <td>...</td>\n","      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n","      <td>Released</td>\n","      <td>NaN</td>\n","      <td>Kahaani</td>\n","      <td>[{'id': 10092, 'name': 'mystery'}, {'id': 1054...</td>\n","      <td>[{'cast_id': 1, 'character': 'Vidya Bagchi', '...</td>\n","      <td>[{'credit_id': '52fe48779251416c9108d6eb', 'de...</td>\n","      <td>16000000.0</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>[{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...</td>\n","      <td>NaN</td>\n","      <td>tt1380152</td>\n","      <td>...</td>\n","      <td>[{'iso_639_1': 'ko', 'name': '한국어/조선말'}]</td>\n","      <td>Released</td>\n","      <td>NaN</td>\n","      <td>Marine Boy</td>\n","      <td>NaN</td>\n","      <td>[{'cast_id': 3, 'character': 'Chun-soo', 'cred...</td>\n","      <td>[{'credit_id': '52fe464b9251416c75073b43', 'de...</td>\n","      <td>3923970.0</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 29 columns</p>\n","</div>"],"text/plain":["   has_homepage  popularity_crew  popularity_cast  index  id  \\\n","0             0              0.0              0.0      0   1   \n","1             0              0.0              0.0      1   2   \n","2             0              0.0              0.0      2   3   \n","3             0              0.0              0.0      3   4   \n","4             0              0.0              0.0      4   5   \n","\n","                               belongs_to_collection    budget  \\\n","0  [{'id': 313576, 'name': 'Hot Tub Time Machine ...  14000000   \n","1  [{'id': 107674, 'name': 'The Princess Diaries ...  40000000   \n","2                                                NaN   3300000   \n","3                                                NaN   1200000   \n","4                                                NaN         0   \n","\n","                                              genres  \\\n","0                     [{'id': 35, 'name': 'Comedy'}]   \n","1  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n","2                      [{'id': 18, 'name': 'Drama'}]   \n","3  [{'id': 53, 'name': 'Thriller'}, {'id': 18, 'n...   \n","4  [{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...   \n","\n","                            homepage    imdb_id  ...  \\\n","0                                NaN  tt2637294  ...   \n","1                                NaN  tt0368933  ...   \n","2  http://sonyclassics.com/whiplash/  tt2582802  ...   \n","3         http://kahaanithefilm.com/  tt1821480  ...   \n","4                                NaN  tt1380152  ...   \n","\n","                                    spoken_languages    status  \\\n","0           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n","1           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n","2           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n","3  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n","4           [{'iso_639_1': 'ko', 'name': '한국어/조선말'}]  Released   \n","\n","                                             tagline  \\\n","0  The Laws of Space and Time are About to be Vio...   \n","1  It can take a lifetime to find true love; she'...   \n","2    The road to greatness can take you to the edge.   \n","3                                                NaN   \n","4                                                NaN   \n","\n","                                      title  \\\n","0                    Hot Tub Time Machine 2   \n","1  The Princess Diaries 2: Royal Engagement   \n","2                                  Whiplash   \n","3                                   Kahaani   \n","4                                Marine Boy   \n","\n","                                            Keywords  \\\n","0  [{'id': 4379, 'name': 'time travel'}, {'id': 9...   \n","1  [{'id': 2505, 'name': 'coronation'}, {'id': 42...   \n","2  [{'id': 1416, 'name': 'jazz'}, {'id': 1523, 'n...   \n","3  [{'id': 10092, 'name': 'mystery'}, {'id': 1054...   \n","4                                                NaN   \n","\n","                                                cast  \\\n","0  [{'cast_id': 4, 'character': 'Lou', 'credit_id...   \n","1  [{'cast_id': 1, 'character': 'Mia Thermopolis'...   \n","2  [{'cast_id': 5, 'character': 'Andrew Neimann',...   \n","3  [{'cast_id': 1, 'character': 'Vidya Bagchi', '...   \n","4  [{'cast_id': 3, 'character': 'Chun-soo', 'cred...   \n","\n","                                                crew     revenue  cast_json  \\\n","0  [{'credit_id': '59ac067c92514107af02c8c8', 'de...  12314651.0              \n","1  [{'credit_id': '52fe43fe9251416c7502563d', 'de...  95149435.0              \n","2  [{'credit_id': '54d5356ec3a3683ba0000039', 'de...  13092000.0              \n","3  [{'credit_id': '52fe48779251416c9108d6eb', 'de...  16000000.0              \n","4  [{'credit_id': '52fe464b9251416c75073b43', 'de...   3923970.0              \n","\n","  crew_json  \n","0            \n","1            \n","2            \n","3            \n","4            \n","\n","[5 rows x 29 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_csv('train.csv', parse_dates=[\"release_date\"])\n","df_test = pd.read_csv('test.csv', parse_dates=[\"release_date\"])\n","df_test.info()\n","df_all = pd.concat([df_train, df_test])\n","df_all.reset_index(inplace=True)\n","\n","# Initialize your new features here\n","df_all['cast_json'] = \"\"\n","df_all['crew_json'] = \"\"\n","df_all.insert(0, 'popularity_cast', np.float64(0))\n","df_all.insert(0, 'popularity_crew', np.float64(0))\n","df_all.insert(0, 'has_homepage', 0)\n","df_all.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJUrl1aSY51U","trusted":true},"outputs":[],"source":["# The JSON payloads are not valid JSON!!\n","# These functions help us parse the invalid JSON to native python objects\n","# Using regular expressions\n","\n","import re\n","def repl_quotes(m):\n","  preq = m.group(1)\n","  qbody = m.group(2)\n","  qbody = re.sub(r'\"', r\"'\", qbody)\n","  return preq + '\"' + qbody + '\"'\n","\n","\n","# Thanks user1384220 of StackOverflow\n","# https://stackoverflow.com/questions/62012736/regex-replace-double-quotes-in-json\n","# Takes an unsafe JSON s, and returns the native py object and safe json string\n","def to_json(s):\n","  safe = s.replace(\"'\", '\"')\n","  safe = re.sub(r'(\"[\\s\\w]*)\"([\\s\\w]*\")',r\"\\1'\\2\", safe)  # O'Brien\n","  safe = re.sub( r'([:\\[,{]\\s*)\"(.*?)\"(?=\\s*[:,\\]}])', repl_quotes, safe ) # Alex \"Nickname\" Schittko\n","  safe = safe.replace(\"None\", 'null')\n","  safe = safe.replace(\"\\\\'\", \"'\")\n","  safe = safe.replace(\"\\\\x92\", \"'\")\n","  safe = safe.replace(\"\\\\xa0\", \"-\")\n","  safe = safe.replace(\"\\\\xad\", \"-\")\n","\n","  #print(safe)\n","  try:\n","    cast_json = json.loads(safe)\n","  except:\n","    print(\"to_json() failed for string\")\n","    print(safe)\n","\n","  return cast_json, safe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUUaviKmovNM","trusted":true},"outputs":[],"source":["# This code splits df_all back to df_test/df_train\n","\n","def split_from_all(df):\n","    df_train = df.iloc[:3000,:]\n","    df_test = df.iloc[3000:,:]\n","    return df_train,df_test\n","\n","# We can iterate df's faster as dicts.\n","def quickIt(df):\n","    data = df.to_dict('index')\n","    idxs = df.index.values\n","    return data, idxs\n"]},{"cell_type":"markdown","metadata":{"id":"1STSsKjQY47f"},"source":[]},{"cell_type":"markdown","metadata":{"id":"lUpRJYIwHJqS"},"source":["# Exploratory Data Analysis (EDA)"]},{"cell_type":"markdown","metadata":{"id":"Fq2CVmjd69QC"},"source":["## Training Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMqxwdu57Gwf","outputId":"72733ee0-6ae8-4b06-d3d8-684a5039874a","trusted":true},"outputs":[],"source":["df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60s188so7HzB","outputId":"f99de6e2-9be4-4df2-b94d-31ff3ae8cd99","trusted":true},"outputs":[],"source":["df_train.info()"]},{"cell_type":"markdown","metadata":{"id":"c4jBjHVl7KpS"},"source":["We can see between from the info output that we have some incomplete features, as well as about 3000 entries to train with.\n","\n","This should be enough for some classification models, however I don't think it will be enough to construct a Neural Network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8IOBvNDHOOL","outputId":"325efd29-1412-4777-90a5-ca746b3e31b9","trusted":true},"outputs":[],"source":["mno.matrix(df_train, (20,6))"]},{"cell_type":"markdown","metadata":{"id":"3aukHKuJ7deT"},"source":["By analyzing the mno \"missing number\" matrix, we can see we need to do something about these incomplete features.\n","\n","* belongs_to_collection\n","* homepage\n","* overview\n","* genres\n","* poster_path\n","* production_companies\n","* production_countries\n","* runtime\n","* spoken_languages\n","* tagline\n","* Keywords\n","* cast\n","* crew\n","\n","We have a few options:\n","\n","1. Discard the feature. We should only do this if we believe the data isn't correlated.\n","2. Impute on the dataset.  We could use IterativeImputer or SimpleImputer to fill in the blanks.\n","3. Feature engineering.  We can extract boolean facts, eg \"has_homepage\" and replace this new feature with the current \"homepage\" feature. This only makes sense for certain features we can turn into classifications.  Eg, the presence of a homepage or tagline may have some influence on the target. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBDwv62XIhRJ","trusted":true},"outputs":[],"source":["# How else do we tell if the data is correlated"]},{"cell_type":"markdown","metadata":{"id":"FX4EebJo3avx"},"source":["### Categorical Features"]},{"cell_type":"markdown","metadata":{"id":"86dRNqWM4Biu"},"source":["#### Overview"]},{"cell_type":"markdown","metadata":{"id":"0rrEfmBhzu6x"},"source":["Here we look at categorical columns in a pie chart to understand the spread of the dataset (original code from [this notebook](https://www.kaggle.com/sisharaneranjana/titanic-survival-prediction-complete-analysis))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfNCYEBg4NdL","outputId":"36684d1f-a2b6-47ba-a798-0ee93f4839e4","trusted":true},"outputs":[],"source":["categorical_cols_train= df_train.select_dtypes(include=['object'])\n","categorical_cols_test= df_test.select_dtypes(include=['object'])\n","print(f'The train dataset contains {len(categorical_cols_train.columns.tolist())} categorical columns')\n","for cols in categorical_cols_train.columns:\n","    print(cols,':', len(categorical_cols_train[cols].unique()),'labels')\n","\n","print(f'The test dataset contains {len(categorical_cols_test.columns.tolist())} categorical columns')\n","for cols in categorical_cols_test.columns:\n","    print(cols,':', len(categorical_cols_test[cols].unique()),'labels')\n","\n","categorical_cols_train.describe()"]},{"cell_type":"markdown","metadata":{"id":"QPRYke_oz_6s"},"source":["The features with > 50 labels are very unique, perhaps we can use specifics about them later in our analysis for correlation.  Perhaps a movie with Danny DeVito has more revenue than one with unheard of actors?\n","\n","It looks like original_language and status are small enough that we could try to see if their values correlated with revenues.\n","\n","We'll need an approach to bring some order to the categorical values before trying to model the problem."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Scalar features\")\n","for col in df_all.columns:\n","    if col not in categorical_cols_train:\n","        print(col)"]},{"cell_type":"markdown","metadata":{"id":"juD0xuk74Cu8"},"source":["#### Status Feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqCujU2v0UB2","outputId":"03c208fa-f338-4a54-d970-5a39572bb315","trusted":true},"outputs":[],"source":["# Checking status' values\n","df_all['status'].unique()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zCAo2Gy23E9","outputId":"e744b045-eb04-473f-982e-1c9f001f9182","trusted":true},"outputs":[],"source":["import plotly.graph_objects as go\n","\n","night_colors = ['#D3DBDD',  'navy',  '#57A7F3']\n","labels = [x for x in df_train.status.value_counts().index]\n","values = df_train.status.value_counts()\n","\n","# Use `hole` to create a donut-like pie chart\n","fig=go.Figure(data=[go.Pie(labels=labels,values=values,hole=.3,pull=[0,0,0.06,0])])\n","\n","fig.update_layout(\n","    title_text=\"Training Set - Movie Status\")\n","fig.update_traces(marker=dict(colors=night_colors))\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxTKkDHb3Q5O","outputId":"6469b0c4-25ab-4a3d-93d3-ba11561e6540","trusted":true},"outputs":[],"source":["\n","night_colors = ['#D3DBDD',  'navy',  '#57A7F3']\n","labels = [x for x in df_test.status.value_counts().index]\n","values = df_test.status.value_counts()\n","\n","# Use `hole` to create a donut-like pie chart\n","fig=go.Figure(data=[go.Pie(labels=labels,values=values,hole=.3,pull=[0,0,0.06,0])])\n","\n","fig.update_layout(\n","    title_text=\"Test Set - Movie Status\")\n","fig.update_traces(marker=dict(colors=night_colors))\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"vB8VcJMu3Vxd"},"source":["Here's a consideration - is Released & Post Production related?"]},{"cell_type":"markdown","metadata":{"id":"3-oUC_ll4I3O"},"source":["# Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"VBOuI4pM8cmz"},"source":["### Collection Sequence Feature\n","\n","We're going to engineer a feature named \"collection_iteration_seq\" that represents which position in a series a movie is. Eg, the 3rd movie in the Dark Knight series will have a value \"3\", where the first movie will have a value \"1\"\n","\n","We'll engineer another feature called \"single\" that will be boolean 0/1, if the movie is a singleton or not.\n","\n","We'll use `df_all` to make sure this feature is complete.\n","\n","After we create the feature on `df_all`, `df_train` and `df_test` will be RECREATED with the new feature.  They'll be split based on having or not having the `revenue` feature."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AHqlTFo8bAl","outputId":"d168431a-89db-4b01-af94-b0cb4bf193f2","trusted":true},"outputs":[],"source":["# Collection Iteration Feature\n","\n","created_collection_iteration = False\n","\n","# First we can iterate over the rows and determine who doesn't belong to a collection.\n","# They'll be the first movies in their Series, for now\n","\n","totalSingle = 0\n","for k, v in df_all.iterrows():\n","    collect = v['belongs_to_collection']\n","    if pd.isna(collect):\n","      #print(\"pd.isna(collect): \" + str(collect))\n","      if not created_collection_iteration:\n","        try:\n","          df_all.insert(k, \"collection_iteration_seq\", 0)\n","          df_all.insert(k, \"single\", 0)\n","          created_collection_iteration = True\n","          df_all.at[k, 'collection_iteration_seq'] = 1\n","          df_all.at[k, 'single'] = 1\n","        except:\n","          df_all.at[k, 'collection_iteration_seq'] = 1\n","          df_all.at[k, 'single'] = 1\n","      else:\n","        df_all.at[k, 'collection_iteration_seq'] = 1\n","        df_all.at[k, 'single'] = 1\n","\n","      totalSingle+=1\n","\n","print(\"Set \" + str(totalSingle) + \" singles\")\n","df_all.info()\n","mno.matrix(df_all, (10,5))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYEY2CcsLhEz","outputId":"d4a14eb0-9b5b-4b61-a68b-9a1762562c5d","trusted":true},"outputs":[],"source":["df_all['collection_iteration_seq'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RuYuUsEy4Nm","trusted":true},"outputs":[],"source":["# Identifying \"Single movies\"\n","\n","collection_ids = []\n","created_collection_id = False\n","\n","moviesInSeries = 0\n","# Iterate all the rows again, and safely read the JSON string in belongs_to_collection\n","all = df_all.to_dict('index')"]},{"cell_type":"markdown","metadata":{"id":"JIk4-KYp_hQz"},"source":["for k in all:\n","  v = all[k]\n","  safe = str(v['belongs_to_collection'])\n","  safe = safe.replace(\"n' \", 'n')\n","  safe = safe.replace(\"'\", '\"')\n","  safe = safe.replace(\"\\\"s \", \"'s\")\n","  safe = safe.replace(\"None\", 'null')\n","  safe = safe.replace(\"N\\\"E\", \"N'E\")\n","  safe = safe.replace(\"We\\\"re\", \"We're\")\n","  safe = safe.replace(\"L\\\"a\", \"L'a\")\n","  if safe != \"nan\": # Only get entries with a belongs_to_collection\n","    parsed = json.loads(safe)\n","    collection_id = parsed[0]['id']\n","    collection_ids.append(collection_id)\n","    # We show here there are only 0 or 1 collection entries on a movie object.\n","    if (len(parsed) > 1):\n","      print(parsed)\n","\n","    if not created_collection_id:\n","      try:\n","        df_all.insert(k, \"collection_id\", 0)\n","        df_all.at[k, 'collection_id'] = collection_id\n","        df_all.at[k, 'single'] = 0\n","        created_collection_id = True\n","      except:\n","        df_all.at[k, 'collection_id'] = collection_id\n","        df_all.at[k, 'single'] = 0\n","    else:\n","      df_all.at[k, 'collection_id'] = collection_id\n","      df_all.at[k, 'single'] = 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88xs8cCFMEEL","outputId":"e1246149-8c36-4f44-d831-6cde2bde3870","trusted":true},"outputs":[],"source":["df_all['collection_iteration_seq'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONBuCeehLn-J","outputId":"5cc8e603-9534-4ad6-a973-bab3c03b9fe1","trusted":true},"outputs":[],"source":["# Identifying series position for movies\n","\n","# Silence Pandas SettingWithCopyWarning\n","pd.options.mode.chained_assignment = None\n","moviesInSeries = 0\n","for cid in collection_ids:\n","    movies = df_all.loc[df_all['collection_id'] == cid]\n","    # SettingWithCopyWarning thrown here but it's ok, we know we're doing this on a copy\n","    # We don't need the copy after we set the counter in the next loop.\n","    movies.sort_values(by='release_date', inplace=True)\n","    \n","    counter = 1\n","    # Apply the value to collection_iteration_seq\n","    for k, v in movies.iterrows():\n","      df_all.at[k, 'collection_iteration_seq'] = counter\n","      counter += 1    \n","      moviesInSeries += 1\n","\n","print(\"Marked collection_iteration_seq on \" + str(moviesInSeries))\n","\n","# Put back Pandas SettingWithCopyWarning\n","pd.options.mode.chained_assignment = \"warn\"\n","mno.matrix(df_all, (10,5))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30NcsvE_MCMZ","outputId":"29e09f05-0dac-474b-cf58-76fcab2827d3","trusted":true},"outputs":[],"source":["df_all['collection_iteration_seq'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KE4RvlUHPKu","trusted":true},"outputs":[],"source":["# now drop collection_id and belongs_to_collection\n","\n","df_all.drop(labels=['belongs_to_collection'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHxQyRWaIHlr","trusted":true},"outputs":[],"source":["# Now create df_train and df_test again\n","df_train,df_test = split_from_all(df_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6D3ep-tIz3j","outputId":"6b883daf-5f4c-4781-dc1d-ff7649c8877e","trusted":true},"outputs":[],"source":["mno.matrix(df_train, (10,5))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOPQbwp0IytS","outputId":"54b88e40-a30c-4730-9775-9a3871ec1333","trusted":true},"outputs":[],"source":["df_train['single'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gplFZ9mDIxUS","outputId":"3e619bf6-2180-4278-c472-a909d8c8a8aa","trusted":true},"outputs":[],"source":["df_train['collection_iteration_seq'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cWVofTseeGn","outputId":"5aba5f8e-aead-4c63-f075-b814f6817bb4","trusted":true},"outputs":[],"source":["df_train.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFu3R6S2ondU","trusted":true},"outputs":[],"source":["# Reduce fragmenting of DataFrame\n","df_all = df_all.copy()"]},{"cell_type":"markdown","metadata":{"id":"GLho4MBr41e8"},"source":[]},{"cell_type":"markdown","metadata":{"id":"eqr4VX_Z45_D"},"source":["### Cast Popularity Feature\n","\n","We're going to build a dataset called `df_cast` that has two features\n","`name` as a key\n","`rating` as a popularity rating\n","\n","Once we have this dataset, we can add a feature called `popularity_cast`, which is a weighted sum/average of the cast's popularity ratings, on the `df_all` set.\n","\n","Afterwards, we should be able to drop the cast feature.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70YlQYTkHy16","outputId":"8b52538d-7ea1-437d-c500-c570fcf69e2d","trusted":true},"outputs":[],"source":["# Here we build a list of all actors\n","\n","actors=[]\n","all = df_all.to_dict('index')\n","for k in all:\n","  v = all[k]\n","  cast_str = v['cast']\n","  if str(cast_str) == \"nan\":\n","    continue\n","  cast_json, safe = to_json(cast_str)\n","  df_all.at[k,'cast_json'] = safe\n","  for actor in cast_json:\n","    #print(actor['name'])\n","    actors.append(actor['name'])\n","df_all.drop(['cast'], axis=1)\n","df_all = df_all.copy()\n","\n","# Now we remove duplicates and create a dataframe to contain our actors\n","\n","actors=list(set(actors))\n","actors_dict=[]\n","for actor in actors:\n","    actors_dict.append({'name':actor,'rating':0,'movies':0})\n","\n","df_cast = pd.DataFrame(actors_dict)\n","df_cast.drop_duplicates(subset=['name'], keep='first')\n","# This speeds us up from 5 frames per second to thousands of frames per second, CPU Only.\n","df_cast.set_index(['name'],inplace=True)\n","df_cast.info()\n","df_cast.index.name"]},{"cell_type":"markdown","metadata":{"id":"fDsLnl9VH9xt"},"source":["Wow!  We have 76k unique actors!  This should give us some good insight!\n","\n","We're going to make a \"rating\" for each actor, then use these \"rating\"s to extract a \"cast_rating\" feature for the films.\n","\n","We'll sum the popularity each film has, on each actor's record.  We'll also keep track of how many films an actor has been in.\n","\n","This lets us average the score of an actor based on the movies they've participated in."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iocjb1uLJ_E-","outputId":"aaa255cd-cd91-474c-a9a7-dcee468d5b50","trusted":true},"outputs":[],"source":["%%time\n","# This part computes the sum of movies & ratings in the df_cast dataframe\n","#dict_cast = df_cast.to_dict('records')\n","all = df_all.to_dict('index')\n","\n","idxs = df_all.index.values\n","for k in tqdm(idxs,desc=\"Computing sums\",unit=\"Film\"):\n","    #print(\"k: \" + str(k))\n","    v = all[k]\n","    cast_json = v['cast_json']\n","    popularity = v['popularity']\n","    #print(cast_json)\n","    #print(popularity)\n","    if str(cast_json) == \"nan\" or str(cast_json) == \"\":\n","      #print(\"bail\")\n","      continue\n","    actors = json.loads(cast_json)\n","    #print(type(actors))\n","    for actor in actors:\n","      #print(actor)\n","      idx = actor['name']\n","      df_cast.at[idx, 'rating'] += popularity\n","      df_cast.at[idx, 'movies'] += 1\n","\n","\n","#print(df_cast[0\n","df_cast.info()\n","df_cast.describe()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91VFfDkMy4Nr","outputId":"d6e3734d-741f-4aac-cd2a-5de6e5664dca","trusted":true},"outputs":[],"source":["# Now we compute the average ratings per actor\n","cast_idxs = df_cast.index.values\n","cast = df_cast.to_dict('index')\n","for k in tqdm(cast_idxs,desc=\"Computing averages\",unit=\"Actor\"):\n","  v = cast[k]\n","  sum_movies = v['movies']\n","  sum_rating = v['rating']\n","  try:\n","    new_rating = sum_rating / sum_movies \n","  except:\n","    new_rating = 0\n","  \n","  df_cast.at[k,'rating'] = new_rating"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTLRJKemy4Nr","outputId":"6d0b3ba0-bfb3-4678-f004-9e0e2f8c05f2","trusted":true},"outputs":[],"source":["df_cast.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_c9Hp36hgah","outputId":"3c21d1a3-96bd-443e-f060-aff1efab4f46","trusted":true},"outputs":[],"source":["# Let's see what the new feature looks like\n","ax = plt.gca()\n","\n","df_cast.plot(kind='scatter',x='movies',y='rating',color='blue',ax=ax)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"SbAEhF68y4Ns"},"source":["So this plot shows us that actors with more movies typically have a lower rating.\n","\n","Now we're going to iterate all the films once more, and engineer this `popularity_cast` rating "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QY2Uwyory4Ns","outputId":"a7bce31a-ba58-4879-936c-d14766db2b2b","trusted":true},"outputs":[],"source":["for idx in tqdm(idxs,desc=\"Comptuing film popularity_cast\",unit=\"Film\"):\n","    film = all[idx]\n","    popularity_cast = 0\n","    count = 0\n","    cast_json = df_all.at[idx,'cast_json']\n","    if cast_json == \"\":\n","        continue\n","    try:\n","        actors = json.loads(cast_json)\n","        if len(actors) > 0:\n","            for actor in actors:\n","                popularity_cast += df_cast.at[actor['name'],'rating']\n","                count+=1\n","    except Exception as e:\n","        print(\"Failed for film\")\n","        print(film)\n","        print(e)\n","        \n","    try:\n","        rating = popularity_cast / count\n","    except:\n","        rating = 0\n","    \n","    if (rating > 100):\n","        print(film['original_title'] + \" \" + str(rating))\n","    \n","    df_all.at[idx,'popularity_cast'] = rating\n","\n","df_all[df_all['original_title'] == 'Minions']['popularity_cast']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cmlxzw3Vy4Ns","outputId":"0a464d0e-6457-4790-ad6e-cecd6e816e41","trusted":true},"outputs":[],"source":["df_all.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZrUPFUCy4Nt","outputId":"00d776b6-512b-4f95-ccd4-6d09d774b676","trusted":true},"outputs":[],"source":["# Let's see what it looks like\n","ax = plt.gca()\n","\n","df_all.plot(kind='scatter',x='revenue',y='popularity_cast',color='blue',ax=ax, figsize=(12,8))\n","\n","plt.show()\n","\n","ax = plt.gca()\n","\n","df_all.plot(kind='scatter',x='revenue',y='popularity',color='red',ax=ax, figsize=(12,8))\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dbUgvXKUy4Nu"},"source":["Nice, this looks like a natural feature now - it's distribution looks similar to that of popularity."]},{"cell_type":"markdown","metadata":{"id":"sDqeQ7Zo5Pz4"},"source":["### Crew Popularity Feature\n","\n","We're going to build a dataset called `df_crew` that has two features\n","`name` as a key\n","`rating` as a popularity rating\n","\n","Once we have this dataset, we can add a feature called \"popularity_crew\" to the movies data sets which is a weighted sum/average of the cast's popularity ratings.\n","\n","Afterwards, we should be able to drop the crew feature.\n","\n","This should pretty much mirror what happened in the cast popularity feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWuUhHQQy4Nu","outputId":"ca5108a1-b0f0-4ca2-e9d0-5723cc9e57ed","trusted":true},"outputs":[],"source":["# Here we build a list of all cast members\n","\n","crews=[]\n","all = df_all.to_dict('index')\n","for k in all:\n","  v = all[k]\n","  crew_str = v['crew']\n","  if str(crew_str) == \"nan\":\n","    continue\n","  crew_json, safe = to_json(crew_str)\n","  df_all.at[k,'crew_json'] = safe\n","  for crew in crew_json:\n","    crews.append(crew['name'])\n","df_all.drop(['crew'], axis=1)\n","df_all = df_all.copy()\n","\n","# Now we remove duplicates and create a dataframe to contain our actors\n","\n","crews=list(set(crews))\n","crews_dict=[]\n","for crew in crews:\n","    crews_dict.append({'name':crew,'rating':0,'movies':0})\n","\n","df_crew = pd.DataFrame(crews_dict)\n","df_crew.drop_duplicates(subset=['name'], keep='first')\n","# This speeds us up from 5 frames per second to thousands of frames per second, CPU Only.\n","df_crew.set_index(['name'],inplace=True)\n","df_crew.info()\n","df_crew.index.name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kV-XMQBDy4Nv","outputId":"197c9082-cba8-4e35-a586-bf0cc730fee5","trusted":true},"outputs":[],"source":["%%time\n","# This part computes the sum of movies & ratings in the df_crew dataframe\n","all = df_all.to_dict('index')\n","\n","idxs = df_all.index.values\n","for k in tqdm(idxs,desc=\"Computing sums\",unit=\"Film\"):\n","    v = all[k]\n","    crew_json = v['crew_json']\n","    popularity = v['popularity']\n","    if str(crew_json) == \"nan\" or str(crew_json) == \"\":\n","      continue\n","    crews = json.loads(crew_json)\n","    for crew in crews:\n","      idx = crew['name']\n","      df_crew.at[idx, 'rating'] += popularity\n","      df_crew.at[idx, 'movies'] += 1\n","\n","df_crew.info()\n","df_crew.describe()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zr3-ByWFy4Nv","outputId":"22832788-2d52-4202-8f8b-3c69160cad2a","trusted":true},"outputs":[],"source":["# Now we compute the average ratings per crew member\n","crew_idxs = df_crew.index.values\n","crew = df_crew.to_dict('index')\n","for k in tqdm(crew_idxs,desc=\"Computing averages\",unit=\"Actor\"):\n","  v = crew[k]\n","  sum_movies = v['movies']\n","  sum_rating = v['rating']\n","  try:\n","    new_rating = sum_rating / sum_movies \n","  except:\n","    new_rating = 0\n","  \n","  df_crew.at[k,'rating'] = new_rating\n","    \n","df_crew.describe()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fBPlyHwy4Nw","outputId":"d1bbaf92-93ac-47ea-bcb8-e361cdf29549","trusted":true},"outputs":[],"source":["ax = plt.gca()\n","\n","df_crew.plot(kind='scatter',x='movies',y='rating',color='blue',ax=ax)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQUxuXIAy4Nw","outputId":"429acb8c-3d85-4996-ceca-37a1a4b6f858","trusted":true},"outputs":[],"source":["for idx in tqdm(idxs,desc=\"Comptuing film popularity_crew\",unit=\"Film\"):\n","    film = all[idx]\n","    popularity_crew = 0\n","    count = 0\n","    crew_json = df_all.at[idx,'crew_json']\n","    if crew_json == \"\":\n","        continue\n","    try:\n","        crews = json.loads(crew_json)\n","        if len(crews) > 0:\n","            for crew in crews:\n","                popularity_crew += df_crew.at[crew['name'],'rating']\n","                count+=1\n","    except Exception as e:\n","        print(\"Failed for film\")\n","        print(film)\n","        print(e)\n","        \n","    try:\n","        rating = popularity_crew / count\n","    except:\n","        rating = 0\n","    \n","    if (rating > 100):\n","        print(film['original_title'] + \" \" + str(rating))\n","    \n","    df_all.at[idx,'popularity_crew'] = rating\n","\n","df_all[df_all['original_title'] == 'Minions']['popularity_crew']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G57_2uo_1ChO","outputId":"943d037d-8af2-416f-9211-5cef57ee93f5","trusted":true},"outputs":[],"source":["# Let's see what it looks like\n","ax = plt.gca()\n","\n","df_all.plot(kind='scatter',x='revenue',y='popularity_crew',color='blue',ax=ax, figsize=(20,8))\n","df_all.plot(kind='scatter',x='revenue',y='popularity_cast',color='red',ax=ax, figsize=(20,8))\n","df_all.plot(kind='scatter',x='revenue',y='popularity',color='green',ax=ax, figsize=(20,8))\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ziVVQF8G5dSv"},"source":["### Keywords Feature\n","\n","We should be able to extract keyword_rating feature like we do for Cast & Crew"]},{"cell_type":"markdown","metadata":{"id":"GtrLIZ955jD8"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLGby4lX45J5","trusted":true},"outputs":[],"source":["### "]},{"cell_type":"markdown","metadata":{"id":"ftZJMeXd5rQb"},"source":["### Homepage Feature\n","\n","We can easily set a boolean for \"has_homepage\" and replace \"homepage\" feature with this"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGXyKDr-y4Nx","trusted":true},"outputs":[],"source":["data, idxs = quickIt(df_all)\n","#print(data)\n","for k in idxs:\n","    v = data[k]\n","    if v['homepage'] == \"\" or str(v['homepage']) == \"nan\":\n","      df_all.at[k,'has_homepage'] = 0\n","    else:\n","      df_all.at[k,'has_homepage'] = 1  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuvyZCvQ4MP6","trusted":true},"outputs":[],"source":["df_all['has_homepage'].unique()\n","df_test.info()\n","df_train, df_test = split_from_all(df_all)\n","df_test.info()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSXpIoGB4UJt","outputId":"6b0f4e73-b727-4caf-99bf-6ce8218040bb","trusted":true},"outputs":[],"source":["ax = plt.gca()\n","\n","df_all.plot(kind='scatter',x='revenue',y='has_homepage',color='blue',ax=ax, figsize=(20,8))\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14FVQ3Tg4LmQ","trusted":true},"outputs":[],"source":["df_all.drop(labels=['homepage'], axis=1, inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYOU8au45WUJ","outputId":"7e33fd2a-fb2a-496a-b1c2-7e3e4b9f0317","trusted":true},"outputs":[],"source":["df_all.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwdaQ25a5d3m","outputId":"8e188a06-7765-4699-9e5e-a80daed6f07b","trusted":true},"outputs":[],"source":["df_all.info()"]},{"cell_type":"markdown","metadata":{"id":"kmbFuswG5xIE"},"source":["### original_language\n","\n","We can replace `original_language` with a dummy because it's unique count is *low*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYxIkvbC5r48","outputId":"25fced68-d701-46aa-bb7e-0da3dc876261","trusted":true},"outputs":[],"source":["len(df_all['original_language'].unique())"]},{"cell_type":"markdown","metadata":{"id":"_GOJxkWV6U4H"},"source":["### spoken_languages\n","\n","not sure what to do with this one"]},{"cell_type":"markdown","metadata":{"id":"zFjQ3v_d6XsT"},"source":["### Status\n","\n","I think any movie in post-production should go to 'released' status to be included in the larger dataset.  Do you?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjwbf78T6eKy"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"z5v1_i1G568z"},"source":["### Drops\n","\n","Leave this last. We drop everything else we don't need for the model here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0pJjuCx6AZx","trusted":true},"outputs":[],"source":["# df_all.drop([\"original_title\",\"cast\",\"crew\",\"cast_json\",\"crew_json\",\"title\",\"imdb_id\"],axis=1,inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Understanding Test Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"k_xQvl_PQ53i"},"source":["# Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88HSWt0TREQq","outputId":"89a4dc0a-b3ca-41ff-8a98-6a8cb381ce03","trusted":true},"outputs":[],"source":["df_train,df_test = split_from_all(df_all)\n","df_train.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIpWljY3RHKz","outputId":"d3a76ec8-fb94-4ceb-c0aa-3d019cb64e80","trusted":true},"outputs":[],"source":["df_test.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mno.matrix(df_test,(20,6))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_test.describe()"]},{"cell_type":"markdown","metadata":{},"source":["### LinearRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQoo6IWDRLiZ","outputId":"9ce8a25d-bc52-460e-81c3-f11466918824","trusted":true},"outputs":[],"source":["# Training the model\n","X = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\n","X.dropna(inplace=True)\n","X.info()\n","y = X['revenue'].to_numpy()\n","X.drop(['revenue'],axis=1,inplace=True)\n","X = X.to_numpy()\n","\n","# This makes it so the model test later sees data it's seen before\n","# This concept stinks, but we want to maximize the amount of data we use for training too.\n","\n","_, X_test, _, y_test = train_test_split(\n","     X, y, test_size=0.33)\n","\n","X_train = X\n","y_train = y\n","\n","import sklearn.ensemble as ske\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.linear_model import LinearRegression\n","\n","train_scores = []\n","lr_models = []\n","max_depth_k = 2 # After about depth 14 the score stays stagnant\n","for k in tqdm(range(1,max_depth_k),desc=\"Training models\",unit=\"LinearRegression\"):\n","  #regr = DecisionTreeRegressor(max_depth=k)\n","  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n","  lr = LinearRegression()\n","  lr.fit(X_train, y_train)\n","  train_scores.append(lr.score(X_train, y_train))\n","  lr_models.append(lr)\n","\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","fig, ax = plt.subplots()\n","xi = np.arange(1, max_depth_k, step=1)\n","y = train_scores\n","\n","plt.ylim(0.0,1.1)\n","plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","plt.xlabel('max_depth')\n","plt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\n","plt.ylabel('Accuracy Score (%)')\n","plt.title('n_estimators for AdaBoostRegressor + base LinearRegressor')\n","\n","plt.axhline(y=0.95, color='r', linestyle='-')\n","plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","plt.axhline(y=0.80, color='r', linestyle='-')\n","plt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n","\n","ax.grid(axis='x')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uC1ZkI4MY-7C","outputId":"120bbd82-d391-4952-93af-c6f50fd2b9f1","trusted":true},"outputs":[],"source":["!pip install pydotplus\n","\n","from sklearn.tree import export_graphviz\n","import pydotplus\n","from IPython.display import Image\n","\n","# This can show a decision tree for RandomForestRegressor\n","#gvz = export_graphviz(selected_model.estimators_[0]) \n","#graph = pydotplus.graph_from_dot_data(gvz) \n","#Image(graph.create_png())"]},{"cell_type":"markdown","metadata":{},"source":["### RandomForestRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training the model\n","X = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\n","X.dropna(inplace=True)\n","X.info()\n","y = X['revenue'].to_numpy()\n","X.drop(['revenue'],axis=1,inplace=True)\n","X = X.to_numpy()\n","\n","# This makes it so the model test later sees data it's seen before\n","# This concept stinks, but we want to maximize the amount of data we use for training too.\n","\n","_, X_test, _, y_test = train_test_split(\n","     X, y, test_size=0.33)\n","\n","X_train = X\n","y_train = y\n","\n","import sklearn.ensemble as ske\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","\n","train_scores = []\n","random_forest_models = []\n","max_depth_k = 21 # After about depth 14 the score stays stagnant\n","for k in tqdm(range(1,max_depth_k),desc=\"Training models\",unit=\"RandomForestRegressor\"):\n","  #regr = DecisionTreeRegressor(max_depth=k)\n","  regr = ske.RandomForestRegressor(max_depth=k)\n","  #regr = ske.AdaBoostRegressor(base_estimator=rfeRegr,n_estimators=100)\n","  regr.fit(X_train, y_train)\n","  train_scores.append(regr.score(X_train, y_train))\n","  random_forest_models.append(regr)\n","\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","fig, ax = plt.subplots()\n","xi = np.arange(1, max_depth_k, step=1)\n","y = train_scores\n","\n","plt.ylim(0.0,1.1)\n","plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","plt.xlabel('max_depth')\n","plt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\n","plt.ylabel('Accuracy Score (%)')\n","plt.title('RandomForestRegressor')\n","\n","plt.axhline(y=0.95, color='r', linestyle='-')\n","plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","plt.axhline(y=0.80, color='r', linestyle='-')\n","plt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n","\n","ax.grid(axis='x')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### DecisionTreeRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training the model\n","X = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\n","X.dropna(inplace=True)\n","X.info()\n","y = X['revenue'].to_numpy()\n","X.drop(['revenue'],axis=1,inplace=True)\n","X = X.to_numpy()\n","\n","# This makes it so the model test later sees data it's seen before\n","# This concept stinks, but we want to maximize the amount of data we use for training too.\n","\n","_, X_test, _, y_test = train_test_split(\n","     X, y, test_size=0.33)\n","\n","X_train = X\n","y_train = y\n","\n","import sklearn.ensemble as ske\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","\n","train_scores = []\n","decision_tree_models = []\n","max_depth_k = 21 # After about depth 14 the score stays stagnant\n","for k in tqdm(range(1,max_depth_k),desc=\"Training models\",unit=\"DecisionTreeRegressor\"):\n","  regr = DecisionTreeRegressor(max_depth=k)\n","  #regr = ske.RandomForestRegressor(max_depth=k)\n","  #regr = ske.AdaBoostRegressor(base_estimator=rfeRegr,n_estimators=100)\n","  regr.fit(X_train, y_train)\n","  train_scores.append(regr.score(X_train, y_train))\n","  decision_tree_models.append(regr)\n","\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","fig, ax = plt.subplots()\n","xi = np.arange(1, max_depth_k, step=1)\n","y = train_scores\n","\n","plt.ylim(0.0,1.1)\n","plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","plt.xlabel('max_depth')\n","plt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\n","plt.ylabel('Accuracy Score (%)')\n","plt.title('DecisionTreeRegressor')\n","\n","plt.axhline(y=0.95, color='r', linestyle='-')\n","plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","plt.axhline(y=0.80, color='r', linestyle='-')\n","plt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n","\n","ax.grid(axis='x')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### AdaBoost + LinearRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training the model\n","X = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\n","X.dropna(inplace=True)\n","X.info()\n","y = X['revenue'].to_numpy()\n","X.drop(['revenue'],axis=1,inplace=True)\n","X = X.to_numpy()\n","\n","# This makes it so the model test later sees data it's seen before\n","# This concept stinks, but we want to maximize the amount of data we use for training too.\n","\n","_, X_test, _, y_test = train_test_split(\n","     X, y, test_size=0.33)\n","\n","X_train = X\n","y_train = y\n","\n","import sklearn.ensemble as ske\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.linear_model import LinearRegression\n","\n","train_scores = []\n","ada_lr_models = []\n","max_depth_k = 2 # After about depth 14 the score stays stagnant\n","for k in tqdm(range(1,max_depth_k),desc=\"Training models\",unit=\"LinearRegression\"):\n","  #regr = DecisionTreeRegressor(max_depth=k)\n","  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n","  lr = LinearRegression()\n","  regr = ske.AdaBoostRegressor(base_estimator=lr)\n","  regr.fit(X_train, y_train)\n","  train_scores.append(regr.score(X_train, y_train))\n","  ada_lr_models.append(regr)\n","\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","fig, ax = plt.subplots()\n","xi = np.arange(1, max_depth_k, step=1)\n","y = train_scores\n","\n","plt.ylim(0.0,1.1)\n","plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","plt.xlabel('max_depth')\n","plt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\n","plt.ylabel('Accuracy Score (%)')\n","plt.title('AdaBoost + Linear Regression')\n","\n","plt.axhline(y=0.95, color='r', linestyle='-')\n","plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","plt.axhline(y=0.80, color='r', linestyle='-')\n","plt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n","\n","ax.grid(axis='x')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### AdaBoost + RandomForest"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# SLOW! ~10-15 minutes\n","# Training the model\n","X = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\n","X.dropna(inplace=True)\n","X.info()\n","y = X['revenue'].to_numpy()\n","X.drop(['revenue'],axis=1,inplace=True)\n","X = X.to_numpy()\n","\n","# This makes it so the model test later sees data it's seen before\n","# This concept stinks, but we want to maximize the amount of data we use for training too.\n","\n","_, X_test, _, y_test = train_test_split(\n","     X, y, test_size=0.33)\n","\n","X_train = X\n","y_train = y\n","\n","import sklearn.ensemble as ske\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","\n","train_scores = []\n","ada_rf_models = []\n","max_depth_k = 14 # After about depth 15 it's useless to keep trying\n","for k in tqdm(range(0,max_depth_k),desc=\"Training models\",unit=\"AdaBoostRegressor\"):\n","  #regr = DecisionTreeRegressor(max_depth=k)\n","  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n","  regr = ske.AdaBoostRegressor(base_estimator=random_forest_models[k])\n","  regr.fit(X_train, y_train)\n","  train_scores.append(regr.score(X_train, y_train))\n","  ada_rf_models.append(regr)\n","\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","fig, ax = plt.subplots()\n","xi = np.arange(0, max_depth_k, step=1)\n","y = train_scores\n","\n","plt.ylim(0.0,1.1)\n","plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","plt.xlabel('Random Forest #')\n","plt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\n","plt.ylabel('Accuracy Score (%)')\n","plt.title('AdaBoost + Random Forest')\n","\n","plt.axhline(y=0.95, color='r', linestyle='-')\n","plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","plt.axhline(y=0.80, color='r', linestyle='-')\n","plt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n","\n","ax.grid(axis='x')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n"]},{"cell_type":"markdown","metadata":{},"source":["### AdaBoost + DecisionTree"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training the model\n","X = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\n","X.dropna(inplace=True)\n","X.info()\n","y = X['revenue'].to_numpy()\n","X.drop(['revenue'],axis=1,inplace=True)\n","X = X.to_numpy()\n","\n","# This makes it so the model test later sees data it's seen before\n","# This concept stinks, but we want to maximize the amount of data we use for training too.\n","\n","_, X_test, _, y_test = train_test_split(\n","     X, y, test_size=0.33)\n","\n","X_train = X\n","y_train = y\n","\n","import sklearn.ensemble as ske\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","\n","train_scores = []\n","ada_dt_models = []\n","max_depth_k = 20 # After about depth 14 the score stays stagnant\n","for k in tqdm(range(0,max_depth_k),desc=\"Training models\",unit=\"AdaBoostRegressor\"):\n","  #regr = DecisionTreeRegressor(max_depth=k)\n","  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n","  regr = ske.AdaBoostRegressor(base_estimator=decision_tree_models[k])\n","  regr.fit(X_train, y_train)\n","  train_scores.append(regr.score(X_train, y_train))\n","  ada_dt_models.append(regr)\n","\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","fig, ax = plt.subplots()\n","xi = np.arange(0, max_depth_k, step=1)\n","y = train_scores\n","\n","plt.ylim(0.0,1.1)\n","plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","plt.xlabel('Decision Tree #')\n","plt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\n","plt.ylabel('Accuracy Score (%)')\n","plt.title('AdaBoost + Decision Tree')\n","\n","plt.axhline(y=0.95, color='r', linestyle='-')\n","plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","plt.axhline(y=0.80, color='r', linestyle='-')\n","plt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n","\n","ax.grid(axis='x')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### GradientBoostRegressor + RandomForest"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training the model\n","X = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\n","X.dropna(inplace=True)\n","X.info()\n","y = X['revenue'].to_numpy()\n","X.drop(['revenue'],axis=1,inplace=True)\n","X = X.to_numpy()\n","\n","# This makes it so the model test later sees data it's seen before\n","# This concept stinks, but we want to maximize the amount of data we use for training too.\n","\n","_, X_test, _, y_test = train_test_split(\n","     X, y, test_size=0.33)\n","\n","X_train = X\n","y_train = y\n","\n","import sklearn.ensemble as ske\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","\n","train_scores = []\n","gb_rf_models = []\n","max_depth_k = 20 # After about depth 14 the score stays stagnant\n","for k in tqdm(range(0,max_depth_k),desc=\"Training models\",unit=\"GradientBoostingRegressor\"):\n","  #regr = DecisionTreeRegressor(max_depth=k)\n","  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n","  regr = ske.GradientBoostingRegressor(init=random_forest_models[k])\n","  regr.fit(X_train, y_train)\n","  train_scores.append(regr.score(X_train, y_train))\n","  gb_rf_models.append(regr)\n","\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","fig, ax = plt.subplots()\n","xi = np.arange(0, max_depth_k, step=1)\n","y = train_scores\n","\n","plt.ylim(0.0,1.1)\n","plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","plt.xlabel('Random Forest #')\n","plt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\n","plt.ylabel('Accuracy Score (%)')\n","plt.title('GradientBoostingRegressor + Random Forest')\n","\n","plt.axhline(y=0.95, color='r', linestyle='-')\n","plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","plt.axhline(y=0.80, color='r', linestyle='-')\n","plt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n","\n","ax.grid(axis='x')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### GradientBoostRegressor + DecisionTree"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Training the model\n","X = df_train.drop(['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index'], axis=1)\n","X.dropna(inplace=True)\n","X.info()\n","y = X['revenue'].to_numpy()\n","X.drop(['revenue'],axis=1,inplace=True)\n","X = X.to_numpy()\n","\n","# This makes it so the model test later sees data it's seen before\n","# This concept stinks, but we want to maximize the amount of data we use for training too.\n","\n","_, X_test, _, y_test = train_test_split(\n","     X, y, test_size=0.33)\n","\n","X_train = X\n","y_train = y\n","\n","import sklearn.ensemble as ske\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","\n","train_scores = []\n","gb_dt_models = []\n","max_depth_k = 20 # After about depth 14 the score stays stagnant\n","for k in tqdm(range(0,max_depth_k),desc=\"Training models\",unit=\"GradientBoostingRegressor\"):\n","  #regr = DecisionTreeRegressor(max_depth=k)\n","  #rfeRegr = ske.RandomForestRegressor(max_depth=k)\n","  regr = ske.GradientBoostingRegressor(init=decision_tree_models[k])\n","  regr.fit(X_train, y_train)\n","  train_scores.append(regr.score(X_train, y_train))\n","  gb_dt_models.append(regr)\n","\n","import matplotlib.pyplot as plt\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","fig, ax = plt.subplots()\n","xi = np.arange(0, max_depth_k, step=1)\n","y = train_scores\n","\n","plt.ylim(0.0,1.1)\n","plt.plot(xi, y, marker='o', linestyle='--', color='b')\n","\n","plt.xlabel('Decision Tree #')\n","plt.xticks(np.arange(0, max_depth_k, step=1)) #change from 0-based array index to 1-based human-readable label\n","plt.ylabel('Accuracy Score (%)')\n","plt.title('GradientBoostingRegressor + Decision Tree')\n","\n","plt.axhline(y=0.95, color='r', linestyle='-')\n","plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n","plt.axhline(y=0.80, color='r', linestyle='-')\n","plt.text(0.5, 0.70, '80% cut-off threshold', color = 'red', fontsize=16)\n","\n","ax.grid(axis='x')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Output"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import datetime\n","\n","def generate_submission(X, model, name):\n","    y_submit = model.predict(X)\n","    submit_list = []\n","    idx = 3001\n","    for y in y_submit:\n","        submit_list.append({'id': idx, 'revenue': y})\n","        idx+=1\n","\n","    submission = pd.DataFrame(submit_list)\n","    timestamp = datetime.datetime.now().isoformat()\n","    sanitized_name = \"\".join([c for c in name if re.match(r'\\w', c)])\n","    submission.to_csv(output_path + sanitized_name + \"-\" + timestamp + \".csv\", index=False)\n","    print(name + \" output available!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["selected_models = [\n","    {\n","        'name': 'RandomForest - max depth 5',\n","        'model': random_forest_models[4]\n","    },\n","    {\n","        'name': 'RandomForest - max depth 12',\n","        'model': random_forest_models[11]\n","    },\n","    {\n","        'name': 'DecisionTree - max depth 6',\n","        'model': random_forest_models[5]\n","    },\n","    {\n","        'name': 'DecisionTree - max depth 11',\n","        'model': decision_tree_models[10]\n","    },\n","    {\n","        'name': 'AdaBoost + RandomForest 8',\n","        'model': ada_rf_models[7]\n","    },\n","    {\n","        'name': 'AdaBoost + RandomForest 11',\n","        'model': ada_rf_models[10]\n","    },\n","    {\n","        'name': 'AdaBoost + DecisionTree 6',\n","        'model': ada_dt_models[5]\n","    },\n","    {\n","        'name': 'AdaBoost + DecisionTree 9',\n","        'model': ada_dt_models[8]\n","    },\n","    {\n","        'name': 'GradientBoostRegressor + RandomForest 5',\n","        'model': gb_rf_models[4]\n","    },\n","    {\n","        'name': 'GradientBoostRegressor + RandomForest 11',\n","        'model': gb_rf_models[10]\n","    },\n","    {\n","        'name': 'GradientBoostRegressor + DecisionTree 5',\n","        'model': gb_rf_models[4]\n","    },\n","    {\n","        'name': 'GradientBoostRegressor + DecisionTree 10',\n","        'model': gb_rf_models[9]\n","    }\n","]\n","\n","print(\"Loaded models for submission\")\n","for m in selected_models:\n","    print(m['model'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","\n","df_test.info()\n","dropped_features = ['single','has_homepage','cast_json','crew_json','crew','cast','Keywords','title','tagline','status','spoken_languages','release_date','production_companies','production_countries','poster_path','overview','original_title','original_language','imdb_id','genres','id','index']\n","df_submit = df_test.drop(dropped_features, axis=1)\n","df_runtime_null = df_submit[df_submit['runtime'].isna()]\n","df_runtime_complete = df_submit[df_submit['runtime'].notna()]\n","\n","runtime_imputer = IterativeImputer(random_state=42)\n","\n","runtime_imputer.fit(df_runtime_complete)\n","runtimes = runtime_imputer.transform(df_runtime_null)\n","#print(runtimes)\n","runtime_null = df_runtime_null.to_dict('index')\n","it = 0\n","for k in runtime_null:\n","    v = runtime_null[k]\n","    df_submit.at[k,'runtime'] = runtimes[it][5]\n","    it+=1\n","    \n","df_submit.info()\n","\n","X_submit = df_submit.drop(['revenue'],axis=1).to_numpy()\n","for m in selected_models:\n","    generate_submission(X_submit, m['model'], m['name'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["timestamp = datetime.datetime.now().isoformat()\n","!zip -r9 outputs-{timestamp}.zip *.csv"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
